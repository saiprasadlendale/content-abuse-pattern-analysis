{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL-rSiBeuI4k",
        "outputId": "7ddf9557-6e82-4701-ebb1-0ab7e9a53278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'jigsaw-toxic-comment-classification-challenge' dataset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9865"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\n",
        "    \"julian3833/jigsaw-toxic-comment-classification-challenge\"\n",
        ")\n",
        "\n",
        "\n",
        "df = pd.read_csv(os.path.join(path, \"train.csv\"))\n",
        "\n",
        "\n",
        "abuse_cols = [\n",
        "    'toxic','severe_toxic','obscene',\n",
        "    'threat','insult','identity_hate'\n",
        "]\n",
        "\n",
        "\n",
        "df['abuse_flag'] = df[abuse_cols].max(axis=1)\n",
        "df['abuse_type_count'] = df[abuse_cols].sum(axis=1)\n",
        "\n",
        "\n",
        "abuse_rate = df['abuse_flag'].mean() * 100\n",
        "abuse_rate\n",
        "\n",
        "df[abuse_cols].sum().sort_values(ascending=False)\n",
        "\n",
        "df[df['abuse_type_count'] > 1].shape[0]\n"
      ]
    }
  ]
}